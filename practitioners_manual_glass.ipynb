{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1-almM0ZF3D"
   },
   "source": [
    "A practitionerâ€™s manual for building LR systems using a data-driven approach\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA5h9E5iaErB"
   },
   "source": [
    "This python notebook accompanies the paper xxx. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlweYWrlUhkN"
   },
   "source": [
    "Follow along in the notebook by running the codeblocks pressing Shift+Enter on your keyboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_t3l27ixzrS"
   },
   "source": [
    "The paper is a guideline to go from a data set to a validated Likelihood Ratio (LR) system in 8 steps. In this notebook, we go through each step using a glass data set, resulting in an LR system for glass. The guideline involves creating multiple LR systems at first, and selection of the best LR system in a later step. In this paper we explore two options to compute scores and two options to transform scores to LRs in a two-by-two design, leading to four LR systems.\n",
    "\n",
    "The 'scorer' options are:\n",
    "1. Use the Manhattan distance (defined as the sum of the absolute differences)\n",
    "2. Use a support vector machine, a machine learning model\n",
    "\n",
    "The 'calibrator' options are:\n",
    "<ol type = 'A'>\n",
    "<li>Use a generative approach, namely kernel density estimation</li>\n",
    "<li>Use a discriminative approach, namely logistic regression</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqED6ypB3tb5"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsabb_dNas5V"
   },
   "source": [
    "### Loading Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hbaU61l9ybL"
   },
   "source": [
    "We start by installing several Python packages. These are standard packages, and the LiR package that we developed and maintain. This package provides a collection of scripts to aid construction and evaluation of LR systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWCy2IyGsuLl",
    "outputId": "30c24795-0583-4ad0-c351-d5aa35851da9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  HAVE_COLAB = True\n",
    "except:\n",
    "  HAVE_COLAB = False\n",
    "\n",
    "print(\"Colab:\", HAVE_COLAB)\n",
    "! pip install scipy==1.7.3\n",
    "if HAVE_COLAB:\n",
    "  ! pip install lir==0.1.21  # dependencies required in Colab, but not locally\n",
    "else:\n",
    "  ! pip install pandas seaborn==0.11.2  # dependencies already installed in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXQtB9-VpXdE"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from random import choices\n",
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import paired_manhattan_distances\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7ewjpFgJ3wH"
   },
   "source": [
    "### Introduction lir and sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Mc2-RutKMzn"
   },
   "source": [
    "LiR follows the conventions of the sklearn package, which is the de facto standard in machine learning. This has the advantage that we can directly use all models defined in sklearn and that the code will look familiar to anyone used to sklearn. It has the disadvantage that naming of certain functions and objects may appear counterintuitive to those unfamiliar with sklearn. Sklearn defines objects that can 'fit' on data, and then 'transform' new data or 'predict' probabilities. LiR additionally allows to 'predict_lr'. We will encounter examples of these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kz9UGBQJ9cH"
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnsFsxCnZ7Ug"
   },
   "source": [
    "We download the glass data in the form of a csv file and convert this to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBR8ua6QaJcR"
   },
   "outputs": [],
   "source": [
    "csv_url = 'https://raw.githubusercontent.com/NetherlandsForensicInstitute/elemental_composition_glass/main/duplo.csv'\n",
    "data_set = pd.read_csv(csv_url, delimiter=',').rename(columns = {'Item': 'Subject', 'Piece': 'Repeat', 'id': 'Id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDMLkAizby_M"
   },
   "source": [
    "# Step 1. Explore data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpK3F2cD1M7-"
   },
   "source": [
    "The data set we will use consists of elemental concentrations of floatglass, obtained with LA-ICPMS (a type of mass spectrometer) ([see github repository](https://github.com/NetherlandsForensicInstitute/elemental_composition_glass)). The elemental concentrations are on a 10 log basis, and normalized to the element Si. For each glass source, which we refer to as *subject*, two measurements are performed. Each time the concentration of ten elements is measured.\n",
    "\n",
    "First, we will explore the type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlC95xzGb7Re",
    "outputId": "7a480288-06e3-49df-a056-ca9098c6f8af"
   },
   "outputs": [],
   "source": [
    "data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me5frqI65Vco"
   },
   "source": [
    "As can be seen there are 13 columns for each observation: \n",
    "- Id: a unique integer for each observation\n",
    "- Subject: a unique integer for each subject\n",
    "- Repeat: for each subject, a unique integer representing the repeated observations on this subject\n",
    "- K39, Ti49, Mn55, Rb85, Sr88, Zr90, Ba137, La139, Ce140, Pb208: the processed concentration of the ten elements. The number behind the element in the column name specifies which isotope of the element was measured. The processed concentrations are all float numbers.\n",
    "\n",
    "Furthermore, we see that there are no non-null values. In other words, there are no missing values.\n",
    "\n",
    "Next, we have a look at the amount of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4_xz-b-ssqi",
    "outputId": "340ccd89-5cbf-41d7-f9b8-3cb20ec43358"
   },
   "outputs": [],
   "source": [
    "print(\"Number of observations: \", data_set['Id'].nunique())\n",
    "print(\"Number of subjects: \", data_set['Subject'].nunique())\n",
    "print(\"Maximum number of repeated observations on one subject: \", data_set['Repeat'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz2uTOprZveI"
   },
   "source": [
    "Above you can see the number of glass panes (=distinct source) and observations.\n",
    "\n",
    "In this manual, we limit the number of items to ensure that we do not run out of RAM during the subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fSj5yITr5NZ"
   },
   "outputs": [],
   "source": [
    "# Make sure that the sorting is first by item, then by repeat\n",
    "data_set = data_set.sort_values(by=[\"Subject\", \"Repeat\"], axis=0, ascending=True)\n",
    "\n",
    "# Only keep the first 400 rows, as the free version of google colab does not have enough RAM available for computations on the full set\n",
    "data_set = data_set.drop(data_set.index[400:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn2pEJfx4dMn"
   },
   "source": [
    "Each row of the dataframe contains one observation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtxblU794T8X",
    "outputId": "fe9ffd61-a3b9-4060-a111-4100bb5a5d6f"
   },
   "outputs": [],
   "source": [
    "print(data_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FMbG0Eh43y6"
   },
   "source": [
    "\n",
    "\n",
    "To get a bit more feeling for the data, we create a histogram of all observations per element together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "BdQeNxW1z2Y0",
    "outputId": "a0cbcfce-14d9-4ea3-a6ce-833e5605c4c6"
   },
   "outputs": [],
   "source": [
    "print(\"Histogram of all measured concentrations:\")\n",
    "data_set[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\", \"Zr90\", \"Ba137\", \"La139\", \"Ce140\",\n",
    "        \"Pb208\"]].hist(figsize=(10,10), layout=(2,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uVcD1E4q3lg"
   },
   "source": [
    "In the histograms, we see that there are no distinctive outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJhlxzAQwOdO"
   },
   "source": [
    "Now, we select the variables (the 10 elements) and labels (indicating which subject was measured) and convert them to a numpy array. This is the format needed for the rest of the steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaG0qs_owLkR"
   },
   "outputs": [],
   "source": [
    "variables = [\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\", \"Zr90\", \"Ba137\", \"La139\", \"Ce140\", \"Pb208\"]\n",
    "labels = [\"Subject\"]\n",
    "\n",
    "obs = data_set[variables].to_numpy()\n",
    "ids = data_set[labels].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZj9_w6xwqVx"
   },
   "source": [
    "### Explore data - a deeper look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0SfjB1EQKPw"
   },
   "source": [
    "Looking at the data more closely, we see that the concentrations within an object appear closer to each other than the concentrations of different objects. To further examine this, we calculate the standard deviation within each glass object and compare it to the standard deviation of samples from different objects. For the latter, we take the mean of the two observations of each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "drE38OKe3HYP",
    "outputId": "8b23ff80-4770-4113-acb5-7de119712b71"
   },
   "outputs": [],
   "source": [
    "print(\"Histograms of standard deviations within objects:\")\n",
    "data_set.groupby('Subject')[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\",\n",
    "                               \"Zr90\", \"Ba137\", \"La139\", \"Ce140\",\n",
    "                               \"Pb208\"]].std().hist(figsize=(10,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "99sydF9LRY3D",
    "outputId": "9a5b80d9-1316-4c73-86ba-4763ea3f7645"
   },
   "outputs": [],
   "source": [
    "print(\"Standard deviations between glass objects compared to the maximum within standard deviation:\")\n",
    "st_devs = pd.DataFrame(data_set.groupby('Subject')[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\",\n",
    "                               \"Zr90\", \"Ba137\", \"La139\", \"Ce140\",\n",
    "                               \"Pb208\"]].mean().std())\n",
    "st_devs.columns = ['std_between']\n",
    "st_devs['std_within_mean'] = pd.DataFrame(data_set.groupby('Subject')[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\",\n",
    "                               \"Zr90\", \"Ba137\", \"La139\", \"Ce140\",\n",
    "                                    \"Pb208\"]].std().mean())\n",
    "st_devs['std_within_max'] = pd.DataFrame(data_set.groupby('Subject')[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\",\n",
    "                               \"Zr90\", \"Ba137\", \"La139\", \"Ce140\",\n",
    "                                    \"Pb208\"]].std().max())\n",
    "st_devs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC6-sTCctnD2"
   },
   "source": [
    "We see that the standard deviations of observations within glass objects roughly lie between 0 and 0.03 (second column), and the standard deviations of observations between objects are much larger (first column). In fact, the between objects standard deviation is larger than the largest with standard deviations we find (third column). This indicates that the output of an LR system based on this data can be discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPF8z6fSmI8w"
   },
   "source": [
    "When different variables are considered for calculating an evidential value, it is best when these variables have correlation close to 0. If two variables are highly correlated however, they can still both be used in the LR system; their combined within variance is reduced compared to measuring a single variable. If the number of variables is large compared to the number of objects, we may consider dimension reduction.\n",
    "\n",
    "To see whether the concentrations of the different elements are correlated, we create scatterplots plotting the concentrations against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v4hBiZOs0bNh",
    "outputId": "44abbbc4-0f8a-4b98-9bda-8ccdacf5d166"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style='ticks')\n",
    "\n",
    "sns.pairplot(data_set.reset_index()[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\", \"Zr90\", \"Ba137\", \"La139\", \"Ce140\", \"Pb208\"]\n",
    "])\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XEXCbkPyCOU"
   },
   "source": [
    "We see that some of the values are correlated, for example for K39 and Rb85. Most scatterplots seem quite random, indicating a low correlation. This is indeed reflected in the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "vrBWuk5Ay9Eh",
    "outputId": "4c1eefdf-4a1b-4d22-da4b-e6528c9c2f44"
   },
   "outputs": [],
   "source": [
    "corr = data_set[[\"K39\", \"Ti49\", \"Mn55\", \"Rb85\", \"Sr88\", \"Zr90\", \"Ba137\", \"La139\", \"Ce140\", \"Pb208\"]].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc93A2Xz0nUU"
   },
   "source": [
    "The results above encourage us to create an LR system based on the processed concentrations of the 10 elements in glass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWsc2kRycbW8"
   },
   "source": [
    "# Step 2. Split data in different subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSwQLDeMf8Nd"
   },
   "source": [
    "\n",
    "As explained in the manuscript, splitting the data is a necessary step in order to evaluate the performance of the model while avoiding overfitting. This means that part of the data will be used to build the model (train and selection sets) and another part is reserved for validation.\n",
    "\n",
    "As shown in the steps below, all three sets should be independent, e.g. consist of disjoint sets of glass objects. First, we set 20% of the objects aside for the validation set, and then we split the remaining objects into training (80%) and selection (20%) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCRVmcST_5da",
    "outputId": "d57fd591-a8e9-4052-ff83-0dcf7e891df5"
   },
   "outputs": [],
   "source": [
    "# First we split off 20% from the data for a hold-out validation set (grouped per glass particle)\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state=1)\n",
    "split = splitter.split(obs, groups=ids)\n",
    "train_select_indices, val_indices = next(split)\n",
    "\n",
    "# Then we split off 20% to use as a test set\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state=1)\n",
    "split = splitter.split(obs[train_select_indices], groups=ids[train_select_indices])\n",
    "train_indices, select_indices = next(split)\n",
    "\n",
    "# We create the train, selection and validation sets\n",
    "# obs are the concentrations, ids are the corresponding labels indicating the source item\n",
    "ids_train = ids[train_indices]\n",
    "ids_select = ids[select_indices]\n",
    "ids_val = ids[val_indices]\n",
    "obs_train = obs[train_indices]\n",
    "\n",
    "\n",
    "# show the sizes of the data sets\n",
    "print(len(obs))\n",
    "\n",
    "print(len(ids_train))\n",
    "print(len(ids_select))\n",
    "print(len(ids_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL5qYIvWvKK6"
   },
   "source": [
    "# Step 3. Pre-process data\n",
    "\n",
    "This step entails transformation of the raw data to informative features that can be used by a statistical model. Standard transformations for concentration data are taking the log10 and normalising by a certain variable (in this particular example: dividing all other variables by the value of Si), both of which are already applied per observation to the glass data we use.\n",
    "\n",
    "To show another example of preprocessing, in addition we here apply a z-score transformation per column to the data set: substracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EfJFfyx0Zf6"
   },
   "outputs": [],
   "source": [
    "z_score_transformer = StandardScaler()\n",
    "z_score_transformer.fit(obs_train)\n",
    "obs_zscore = z_score_transformer.transform(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAOrm5YJMeO-"
   },
   "source": [
    "We can see what this transformation does for one of the columns in the figures below. The distribution is scaled and shifted, but otherwise the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "3IEuHZV-G3D4",
    "outputId": "111ebed5-292b-4975-f0d8-de5b871cf72d"
   },
   "outputs": [],
   "source": [
    "plt.hist(obs[:,0], bins = 20);\n",
    "plt.title('K39 values before pre-processing')\n",
    "plt.figure()\n",
    "plt.hist(obs_zscore[:,0], bins = 20);\n",
    "plt.title('K39 values after pre-processing');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQjQHbFuvfCU"
   },
   "source": [
    "# Step 4. Calculate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iiy_7IXZT59F"
   },
   "source": [
    "In the previous steps we have explored the given data, split it into subsets and performed pre-processing. We call the 10 pre-processed values we now have for each observation the features. The next step is to go from these features to a score. In order to do this we have to make pairs of observations, both *H*<sub>1</sub>-true and *H*<sub>2</sub>-true, and calculate the scores based on their features. A score is meant to quantify the degree of (dis)similarity between a pair of observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3lGrLfwRDC7"
   },
   "source": [
    "### Create pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd0INeH08pYK"
   },
   "source": [
    "The function `create_pairs` below creates one *H*<sub>1</sub>-true pair per subject, and one *H*<sub>2</sub>-true pair for each pair of different subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAwb3gWRcy8y"
   },
   "outputs": [],
   "source": [
    "def create_pairs(obs, ids):\n",
    "    \"\"\"\n",
    "    creates all possible pairs between the items represented by ids\n",
    "    the ids refer to the total set of observations given by obs.\n",
    "    \"\"\"\n",
    "    # Create matrices with item IDs that refer to the total data set\n",
    "    H1_ids = np.transpose(np.tile(np.unique(ids), (2, 1)))\n",
    "    H2_ids = np.asarray(list(combinations(np.unique(ids), 2)))\n",
    "\n",
    "    # For H1-data: use the first repeat for each item in the first colum,\n",
    "    # and the second repeat of that item in the second.\n",
    "    # It is assumed: that obs is sorted first by item ID, then by repeat ID; \n",
    "    # that all items have exactly 2 repeats; that there are no missing items.\n",
    "    H1_obs_rep_1 = obs[2*H1_ids[:,0] - 2]\n",
    "    H1_obs_rep_2 = obs[2*H1_ids[:,1] - 1]\n",
    "    H1_obs_pairs = np.stack((H1_obs_rep_1, H1_obs_rep_2), axis=2)\n",
    "\n",
    "    # For H2-data: use for both items their first repeats\n",
    "    H2_obs_item_1 = obs[2*H2_ids[:,0] - 2]\n",
    "    H2_obs_item_2 = obs[2*H2_ids[:,1] - 2]\n",
    "    H2_obs_pairs = np.stack((H2_obs_item_1, H2_obs_item_2), axis=2)\n",
    "\n",
    "    # Combine the H1 and H2 data, and create vector with classes: H1=1 & H2=0\n",
    "    obs_pairs = np.concatenate((H1_obs_pairs, H2_obs_pairs))\n",
    "    H1_same_source = np.ones(H1_ids.shape[0])\n",
    "    H2_same_source = np.zeros(H2_ids.shape[0])\n",
    "    same_source = np.concatenate((H1_same_source, H2_same_source))\n",
    "\n",
    "    return obs_pairs, same_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWtoFN6LSKRi"
   },
   "source": [
    "We create the pairs for the subjects in the train data and the selection data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBRh4jaumhBJ"
   },
   "outputs": [],
   "source": [
    "obs_pairs_train, same_source_train = create_pairs(obs_zscore, ids_train)\n",
    "obs_pairs_select, same_source_select = create_pairs(obs_zscore, ids_select)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okU1f1mLdGlU"
   },
   "source": [
    "Once we have the same-source and different-source pairs we have to compute the score per pair. We show two different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reYmV7RrdIGL"
   },
   "source": [
    "### Option 1: Compute (dis)similarity score \n",
    "We take the Manhattan distance, also called *L*<sub>1</sub>-norm. This is defined as the of the absolute differences of the features. Luckily, distances such as these are standard functions in the python package sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RupTP9OLmpb"
   },
   "outputs": [],
   "source": [
    "dissimilarity_scores_train = paired_manhattan_distances(obs_pairs_train[:,:,0], obs_pairs_train[:,:,1])\n",
    "dissimilarity_scores_select = paired_manhattan_distances(obs_pairs_select[:,:,0], obs_pairs_select[:,:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhx5M4K_MN1V"
   },
   "source": [
    "### Option 2: Compute machine learning score\n",
    "Alternatively, we can fit a statistical model to compute scores. In this case, we use a support vector machine. First the absolute differences for each elemental value between the pairs of observations are computed. The support vector machine then assigns a score to each pair. The model aims to assign high scores to *H*<sub>1</sub>-true pairs and low scores to *H*<sub>2</sub>-true pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVYg821mJsGl"
   },
   "outputs": [],
   "source": [
    "from lir.transformers import AbsDiffTransformer\n",
    "\n",
    "# machine learning models need a single vector as input. The AbsDiffTransformer takes two feature vectors, one for each subject of a pair, and returns the elementwise absolute differences\n",
    "# The AbsDiffTransformer and support vector machine (SVC) are combined into a single pipeline using sklearns Pipeline class.\n",
    "machine_learning_scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "# the model has to be fit on the data\n",
    "machine_learning_scorer.fit(obs_pairs_train, same_source_train)\n",
    "# score can be computed using the 'predict_proba' function. This is another sklearn convention, which returns two columns of which we take the second using '[:,1]'\n",
    "machine_learning_scores_train = machine_learning_scorer.predict_proba(obs_pairs_train)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDPOXCifSuUS"
   },
   "source": [
    "The scores look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "pK_qh0F_Sw5T",
    "outputId": "a32c4e84-7497-4b01-aa15-ea4065f5134c"
   },
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.hist(dissimilarity_scores_train[same_source_train==1], density=True, bins=range(30))\n",
    "plt.hist(dissimilarity_scores_train[same_source_train==0], density=True, bins=range(30));\n",
    "plt.title('Dissimilarity scores for (blue) H1-true and (orange) H2-true pairs');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "hsd8jX3YTzqJ",
    "outputId": "ae0a4d54-0667-4ac9-c6ae-4312b5c69332"
   },
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.hist(machine_learning_scores_train[same_source_train==1], density=True, bins=np.linspace(0, 1, 10))\n",
    "plt.hist(machine_learning_scores_train[same_source_train==0], density=True, bins=np.linspace(0, 1, 10));\n",
    "plt.title('Machine learning scores for (blue) H1-true and (orange) H2-true pairs');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29FnyNipU05Z"
   },
   "source": [
    "As expected, the *H*<sub>1</sub>-true pairs generally have lower dissimilary scores than the *H*<sub>2</sub>-true pairs, while their machine learning scores are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVUFNWS8vfVb"
   },
   "source": [
    "#Step 5. Calculate LRs from scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W1c8Cj6dKbY"
   },
   "source": [
    "After step 4 we have a score for each pair. Two approaches exist to convert these scores to LRs. We show an example for each approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWzm5bP9gXEC"
   },
   "source": [
    "### Generative approach\n",
    "\n",
    "In the generative approach approach, we model the distribution of scores under *H*<sub>1</sub> and under *H*<sub>2</sub>. Here, we use kernel density estimation (KDE) to do this. Lir defines classes called 'Calibrators' to perform the score to LR mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KbSziDavfcq"
   },
   "outputs": [],
   "source": [
    "kde_calibrator = lir.KDECalibrator(bandwidth='silverman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "femRMlnRoPQS"
   },
   "source": [
    "To illustrate how this works, we plot the histogram for the scores under *H*<sub>1</sub> and *H*<sub>2</sub>-true, together with the KDE fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "V1EH61o0oNba",
    "outputId": "e07190bc-a859-404f-c492-02b13d46c8d3"
   },
   "outputs": [],
   "source": [
    "kde_calibrator.fit(dissimilarity_scores_train, same_source_train)\n",
    "with lir.plotting.show() as ax:\n",
    "    ax.calibrator_fit(kde_calibrator, score_range=[0,30]);\n",
    "    ax.hist(dissimilarity_scores_train[same_source_train==1], density=True, bins=range(30), color='C0');\n",
    "    ax.hist(dissimilarity_scores_train[same_source_train==0], density=True, bins=range(30), color='C1');\n",
    "    ax.title('KDE fit on the dissimilarity scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVfefhuWA4Nn"
   },
   "source": [
    "### Discriminative approach\n",
    "Alternatively, we can use other discriminative methods. Here we use logistic regression to map the score to posterior odds. This can be corrected for the amount of *H*<sub>1</sub> and *H*<sub>2</sub> data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nii8tmG8g_T7"
   },
   "outputs": [],
   "source": [
    "logreg_calibrator = lir.LogitCalibrator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AJrm1R-YZN-"
   },
   "source": [
    "### Bounding the LRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtD_6oAQpKyY"
   },
   "source": [
    "The LR system may output values that seem too large or too small given the size of our data set. Several methods have been proposed mitigate this problem, by 'shrinking' the LRs towards 1. Here we show how to use empirical lower and upper bounds (ELUB) to limit the minimum and maximum values the LR system can output. Using lir, we can stack this 'ELUBbounder' on any calibrator we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2Qjs4lNh1Oq"
   },
   "outputs": [],
   "source": [
    "bounded_kde_calibrator = lir.ELUBbounder(kde_calibrator)\n",
    "bounded_logreg_calibrator = lir.ELUBbounder(logreg_calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y0ANL6KohdA"
   },
   "source": [
    "### Illlustration of scores to LRs\n",
    "To illustrate the score to LR mapping, we plot how the dissimilarity scores are mapped to LRs (using a log scale). We print the values of the ELUB bounds - these are visible in the plot as the minimum and maximum values the LRs obtain (horizontal lines).\n",
    "\n",
    "One thing to note is that the blue line (logistic regression) never goes up. This is a desirable property, it makes sense that more extreme scores correspond to more extreme LRs. In contrast, the orange line (KDE) shows several humps, e.g. at score=0 and score=2. This means that counterintuitively, sometimes a higher dissimilarit leads to a lower LR. Here we don't 'fix' this (e.g. by adjusting the bandwidth of the KDE), but be aware that some methods for transforming scores to LRs can have this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "hcNgO354aNRP",
    "outputId": "267b49c5-e798-41e4-d3af-df4ca2f6ac54"
   },
   "outputs": [],
   "source": [
    "lrs_train_logreg = bounded_logreg_calibrator.fit_transform(dissimilarity_scores_train, same_source_train)\n",
    "lrs_train_kde = bounded_kde_calibrator.fit_transform(dissimilarity_scores_train, same_source_train)\n",
    "\n",
    "plt.scatter(dissimilarity_scores_train, np.log10(lrs_train_logreg), label='logistic regression')\n",
    "plt.scatter(dissimilarity_scores_train, np.log10(lrs_train_kde), label = 'kernel density estimation (KDE)')\n",
    "plt.xlim([0,5])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Dissimilarity score')\n",
    "plt.ylabel('log10 LR')\n",
    "print(f'ELUB log LR bounds for logreg are {np.log10(bounded_logreg_calibrator._lower_lr_bound):.2f} and {np.log10(bounded_logreg_calibrator._upper_lr_bound):.2f}')\n",
    "print(f'ELUB log LR bounds for kde are {np.log10(bounded_kde_calibrator._lower_lr_bound):.2f} and {np.log10(bounded_kde_calibrator._upper_lr_bound):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm940saTvtoH"
   },
   "source": [
    "# Step 6. Select best LR system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnvy3CU1yhF2"
   },
   "source": [
    "In this step, we compute the performance of each LR system defined on the selection data. For this we use the CalibratedScorer class from lir, which allows you to combine computing scores and transforming to LRs in one step. A CalibratedScorer needs two parts: a function or machine learning model to compute scores, and a Calibrator to transform the scores to LRs\n",
    "\n",
    "We use the following two plots and metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozHVgS_YUXWG"
   },
   "outputs": [],
   "source": [
    "def show_performance(lrs, same_source, calibrator):\n",
    "  # show the distribution of LRs together with the ELUB values\n",
    "  with lir.plotting.show() as ax:\n",
    "      ax.lr_histogram(lrs, same_source)\n",
    "      ax.title('Histogram of H1-true (orange) and H2-true (blue) LRs')\n",
    "\n",
    "  print(f'\\n ELUB log LR bounds are {np.log10(calibrator._lower_lr_bound):.2f} and {np.log10(calibrator._upper_lr_bound):.2f} \\n')\n",
    "\n",
    "  # show the PAV plot (closer to the line y=x is better)\n",
    "  with lir.plotting.show() as ax:\n",
    "      ax.pav(lrs, same_source)\n",
    "\n",
    "  # print the quality of the system as log likelihood ratio cost (lower is better)\n",
    "  print(f'\\n The log likelihood ratio cost is {lir.metrics.cllr(lrs, same_source):.3f} (lower is better)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIbmOuuXjBwD"
   },
   "source": [
    "### Option 1: Manhattan distance + KDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "raEanCwsvfto",
    "outputId": "aab28b6b-464f-4e07-a4a2-dbb0b22882e4"
   },
   "outputs": [],
   "source": [
    "# we define the CalibratedScorer. We use the standard sklearn function paired_manhattan_distance to compute the scores,\n",
    "# and an ELUB bounded KDE calibrator to transform to LRs.\n",
    "lr_system = lir.CalibratedScorer(paired_manhattan_distances, bounded_kde_calibrator)\n",
    "# We fit the entire lr system\n",
    "lr_system.fit(obs_pairs_train, same_source_train)\n",
    "\n",
    "# and compute the LRs on the selection data\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "# this is how well the system performs\n",
    "show_performance(lrs_select, same_source_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KOiJLt2qdDx"
   },
   "source": [
    "### Option 2: Manhattan distance + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "sjj5RdAHqWv5",
    "outputId": "e6a344a1-6cdc-48ee-85f2-3329c4d935e1"
   },
   "outputs": [],
   "source": [
    "# we define the CalibratedScorer. We use the standard sklearn function paired_manhattan_distance to compute the scores,\n",
    "# and an ELUB bounded logistic regression calibrator to transform to LRs.\n",
    "lr_system = lir.CalibratedScorer(paired_manhattan_distances, bounded_logreg_calibrator)\n",
    "# We fit the entire lr system\n",
    "lr_system.fit(obs_pairs_train, same_source_train)\n",
    "\n",
    "# and compute the LRs on the selection data\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "# this is how well the system performs\n",
    "show_performance(lrs_select, same_source_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eUrLzX2rqfZ"
   },
   "source": [
    "### Option 3: support vector machine + KDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "JePlY8kztDjv",
    "outputId": "b767440e-24d3-4f3e-afe8-ddca9b12e7c1"
   },
   "outputs": [],
   "source": [
    "# The whole LR system consists of computing the score and then mapping the scores to lrs\n",
    "# we use the machine learning scorer introduced in step 4\n",
    "scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "lr_system = lir.CalibratedScorer(scorer, bounded_kde_calibrator)\n",
    "\n",
    "# we fit the whole system. When fitting (=training) a CalibratedScorer, both the machine learning model and the transformation to LRs are trained on the supplied data\n",
    "lr_system.fit(obs_pairs_train, same_source_train)\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "\n",
    "show_performance(lrs_select, same_source_select, lr_system.calibrator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu4Bcbsss_fI"
   },
   "source": [
    "### Option 4: support vector machine + logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "xs47hMySrmqG",
    "outputId": "da21c398-7ba8-4523-dac0-c9e60fbc37bb"
   },
   "outputs": [],
   "source": [
    "# The whole LR system consists of computing the score and then mapping the scores to lrs\n",
    "scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "lr_system = lir.CalibratedScorer(scorer, bounded_logreg_calibrator)\n",
    "# we fit the whole system\n",
    "lr_system.fit(obs_pairs_train, same_source_train)\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "show_performance(lrs_select, same_source_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YljF4uSbuHMI"
   },
   "source": [
    "### Final selection\n",
    "All four options show reasonable LR distributions. Because the glass data contain much information the *H*<sub>1</sub> and *H*<sub>2</sub> are perfectly separated. Most of the *H*<sub>2</sub> pairs, and many of the *H*<sub>1</sub> pairs fall on the ELUB bounds. We see similar performance across the four systems. The LR distributions look reasonable for all systems, the PAV plots look good for options 2 and 3 (with the points following the line y=x) and show slightly conservative LRs for options 1 and 4 (points lying above the line y=x).\n",
    "\n",
    "We have a slight preference for option 3, which has a good PAV plot, the best (=lowest) *C*<sub>llr</sub> and best (=widest) ELUB bounds. Option 2 would also have been a reasonable choice, particularly as the Manhattan distance is much more explainable than the support vector machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyaszrzmvf1r"
   },
   "source": [
    "# Step 7. Validate selected LR system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wNu8nK75EcP"
   },
   "source": [
    "After selecting the (best) model, you should assess the performance of the system on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUmGVlDO_GbZ"
   },
   "source": [
    "### Construct the selected system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu4twJ90_N54"
   },
   "source": [
    "Since we want to build the final model based on as much data as possible, we combine the train and selection set and use this to train the final LR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU1Ee4PxSOBa",
    "outputId": "8336ecd9-3619-4f19-db3a-ee1ed9f6ec2e"
   },
   "outputs": [],
   "source": [
    "# create the combined data set\n",
    "obs_train_select = obs[train_select_indices]\n",
    "ids_train_select = ids[train_select_indices]\n",
    "\n",
    "# step 3 pre-processing: normalise\n",
    "z_score_transformer.fit(obs_train_select)\n",
    "obs_zscore = z_score_transformer.transform(obs)\n",
    "\n",
    "# step 4: combine the pairs into one feature vector by taking the absolute difference\n",
    "obs_pairs_train_select, same_source_train_select = create_pairs(obs_zscore, ids_train_select)\n",
    "obs_pairs_val, same_source_val = create_pairs(obs_zscore, ids_val)\n",
    "\n",
    "selected_lr_system = lir.CalibratedScorer(machine_learning_scorer, lir.ELUBbounder(lir.KDECalibrator()))\n",
    "# step 4+5 combined: we fit the whole system\n",
    "selected_lr_system.fit(obs_pairs_train_select, same_source_train_select)\n",
    "\n",
    "# compute the LRs on the validation data\n",
    "lrs_val = selected_lr_system.predict_lr(obs_pairs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4z8tJgF_P54"
   },
   "source": [
    "### Calculate system performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "a5BmurxZvf-8",
    "outputId": "46d5884d-9ac7-4e6c-f335-85304f67c1ea"
   },
   "outputs": [],
   "source": [
    "# we always inspect the characteristics we also look at in selection\n",
    "show_performance(lrs_val, same_source_val, selected_lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "NgGqN6o46BqD",
    "outputId": "3f90f5f4-37fb-4010-ccec-5aec51100c6e"
   },
   "outputs": [],
   "source": [
    "# There are many other characteristics that we may want to inspect, such as the empirical cross entropy (ECE) plot.\n",
    "with lir.plotting.show() as ax:\n",
    "      ax.ece(lrs_val, same_source_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUWNjDBL_Cez"
   },
   "source": [
    "In the validation, we hope to see performance comparable to that on the selection data. This is the case here, with  *C*<sub>llr</sub>, ELUB bounds and LR distributions looking very similar to those we saw in step 6. The PAV plot looks reasonable, with the data points mostly following the line y=x. There are a few datapoint that lie under this line, indicating a slight overconfidence of the system in this LR range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foIk-OxuOg1A"
   },
   "source": [
    "# Step 8. Construct casework LR system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er-ykWWV_fEI"
   },
   "source": [
    "To obtain our casework LR system, we fit on all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AditaVUo-1DU"
   },
   "outputs": [],
   "source": [
    "# create new pairs for the combined data\n",
    "obs_zscore = z_score_transformer.fit_transform(obs)\n",
    "obs_pairs, same_source = create_pairs(obs_zscore, ids)\n",
    "\n",
    "# fit the same system on all the data\n",
    "selected_lr_system.fit(obs_pairs, same_source);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z37mueAI_9pw"
   },
   "source": [
    "Note that we can not empirically test this system. In casework, if you have decided to use the system, you would input your pair of observation and get back an LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb40EneuALen",
    "outputId": "412be76c-94e3-4b42-b8d1-79a673d2fb61"
   },
   "outputs": [],
   "source": [
    "observation_1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "observation_2 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "manual_entered_data = np.moveaxis(np.array([z_score_transformer.transform([observation_1,observation_2])]),1,-1)\n",
    "\n",
    "print(f'The LR obtained is {selected_lr_system.predict_lr(manual_entered_data)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gph0ui5uvgSc"
   },
   "source": [
    "# TODO Putting it all together (a full pipeline?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQds_2TXj1vG"
   },
   "source": [
    "In the notebook so far we have separated the different steps. In practice, we use code that is more concise and readable, which also allows us to automatically assess the different options we have defined. This section shows how to do that."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2KOiJLt2qdDx"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
